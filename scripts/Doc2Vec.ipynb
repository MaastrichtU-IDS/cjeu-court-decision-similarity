{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUR-LEX Case Similarity Notebook: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract: \n",
    "#### This notebook implements Doc2Vec document similarity measures (based on Word2Vec) on EUR-LEX judgements and orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import data & resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.e. import full texts of cases from file, define a mapping to case IDs so we can lookup Doc2Vec similarity values by caseID, import stopwords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List all documents in directory\n",
    "path = \"../inputdata/full_texts_all_cases/\"\n",
    "\n",
    "# Import stopwords           \n",
    "stopwordsfile = \"../script_resources/stopwords.pickle\"\n",
    "stopwords_full = []\n",
    "with open(stopwordsfile, \"rb\") as f:\n",
    "    tmp = pickle.load(f)\n",
    "    stopwords_full.extend(list(tmp))\n",
    "    stopwords_full.extend(stopwords.words('english'))\n",
    "    \n",
    "stopwords_full = list(set(stopwords_full))\n",
    "\n",
    "# Only keep celex number from filename\n",
    "def cleanfilename(name):\n",
    "    result = \"\"\n",
    "    result = name.replace(\"full_text_\",\"\")\n",
    "    result = result.replace(\".txt\",\"\")\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    global stopwords_full\n",
    "    for item in stopwords_full:\n",
    "        text = text.replace(str(item),'')\n",
    "    return text\n",
    "\n",
    "# Import files and define mapping between case IDS and full texts   \n",
    "files = []\n",
    "index_to_celex = {}\n",
    "datafortraining = []\n",
    "index = 0\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "            celexnum = cleanfilename(os.path.basename(file))\n",
    "            with open (path+file, \"r\", encoding=\"utf-8\" ) as myfile:\n",
    "                data = myfile.read().replace('\\n', '')\n",
    "                data = remove_stopwords(data)\n",
    "                datafortraining.append(data)\n",
    "                index_to_celex[index] = file\n",
    "                index += 1\n",
    "                \n",
    "documents = [TaggedDocument(file, [i]) for i, file in enumerate(datafortraining)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Train Doc2Vec model on input case texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while... a few hours\n",
    "model = Doc2Vec(documents, vector_size=32, window=5, min_count=1, workers=4)\n",
    "model.train(documents, total_examples=model.corpus_count,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Import sample cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch sample cases from file\n",
    "def get_sample_cases(topic):\n",
    "    data = pd.read_csv(\"../inputdata/sampled_cases.csv\")\n",
    "    relevant_rows = data[data['topic'] == topic]\n",
    "    return relevant_rows['source'].tolist()\n",
    "\n",
    "# Celex numbers of reference cases\n",
    "publichealth = get_sample_cases('public health')\n",
    "socialpolicy = get_sample_cases('social policy')\n",
    "dataprotection = get_sample_cases('data protection')\n",
    "\n",
    "print(publichealth)\n",
    "print(socialpolicy)\n",
    "print(dataprotection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Look up top n similar cases per sample case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfile(celexnumber):\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if celexnumber in file:\n",
    "                return file\n",
    "    return None\n",
    "\n",
    "def celex_to_index(celexnumber):\n",
    "    file = getfile(celexnumber)\n",
    "    for k, v in index_to_celex.items():\n",
    "        if v == file:\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "results = []\n",
    "# 1. Public Health\n",
    "for item in publichealth:\n",
    "    similar_docs = model.docvecs.most_similar(celex_to_index(item), topn=20)\n",
    "    print(similar_docs)\n",
    "#     index = get_doc_index(item)                         # Look up this cases index in the TFIDF matrix\n",
    "#     similar_cases = find_similar(tfidf_data, index, 20) # Look up top 20 similar cases for this case\n",
    "#     similar_cases_references = convert_to_case_references(similar_cases)\n",
    "#     for reference in similar_cases_references:\n",
    "#         results.append([item,reference[0],reference[1],'public health'])\n",
    "\n",
    "# # 2. Social Policy\n",
    "# for item in socialpolicy:\n",
    "#     similar_docs = model.docvecs.most_similar(celex_to_index(item), topn=20)\n",
    "# #     index = get_doc_index(item)                         # Look up this cases index in the TFIDF matrix\n",
    "# #     similar_cases = find_similar(tfidf_data, index, 20) # Look up top 20 similar cases for this case\n",
    "# #     similar_cases_references = convert_to_case_references(similar_cases)\n",
    "# #     for reference in similar_cases_references:\n",
    "# #         results.append([item,reference[0],reference[1],'social policy'])\n",
    "\n",
    "# # 3. Data Protection\n",
    "# for item in dataprotection:\n",
    "#     similar_docs = model.docvecs.most_similar(celex_to_index(item), topn=20)\n",
    "# #     index = get_doc_index(item)                         # Look up this cases index in the TFIDF matrix\n",
    "# #     similar_cases = find_similar(tfidf_data, index, 20) # Look up top 20 similar cases for this case\n",
    "# #     similar_cases_references = convert_to_case_references(similar_cases)\n",
    "# #     for reference in similar_cases_references:\n",
    "# #         results.append([item,reference[0],reference[1],'data protection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"The most similar 2 document(s) to \" + index_to_celex[1] + \" are: \")\n",
    "for item in similar_docs:\n",
    "    print(index_to_celex[item[0]])\n",
    "#print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# #model = KeyedVectors.load_word2vec_format('Embeddings\\\\lemmatized-legal\\\\NNP replaced\\\\legallemmatextreplacewithnnp.bin', binary=True, unicode_errors='ignore')\n",
    "# model = KeyedVectors.load_word2vec_format('Embeddings\\\\Law2Vec.200d.txt', binary=False, unicode_errors='ignore')\n",
    "# model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "# # Access vectors for specific words with a keyed lookup:\n",
    "# # vector = model['easy']\n",
    "# # see the shape of the vector (300,)\n",
    "# # print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import documents and stopwords\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# # Only keep celex number from filename\n",
    "# def cleanfilename(name):\n",
    "#     result = \"\"\n",
    "#     result = name.replace(\"full_text_\",\"\")\n",
    "#     result = result.replace(\".txt\",\"\")\n",
    "#     return result\n",
    "\n",
    "# # List all documents in directory\n",
    "# path = \"Data\\\\case-law-analysis\\\\data\\\\casescontent\\\\rulings\\\\\"\n",
    "\n",
    "# files = []\n",
    "# file_dict = {}\n",
    "# # r=root, d=directories, f = files\n",
    "# for r, d, f in os.walk(path):\n",
    "#     for file in f:\n",
    "#         if '.txt' in file:\n",
    "#             files.append(os.path.join(r, file))\n",
    "#             celexnum = cleanfilename(os.path.basename(file))\n",
    "#             with open (path+file, \"r\", encoding=\"utf-8\" ) as myfile:\n",
    "#                 data = myfile.read().replace('\\n', '')\n",
    "#                 file_dict[celexnum] = data\n",
    "\n",
    "# print(len(file_dict))\n",
    "\n",
    "# values = []\n",
    "# key = {}\n",
    "# counter = 0\n",
    "# for k,v in file_dict.items():\n",
    "#     values.append(v)\n",
    "#     key[k] = counter\n",
    "#     counter+=1\n",
    "    \n",
    "# print(len(values))\n",
    "            \n",
    "# stopwordsfile = \"stopwords.pickle\"\n",
    "# stopwords = []\n",
    "# with open(stopwordsfile, \"rb\") as f:\n",
    "#     tmp = pickle.load(f)\n",
    "#     stopwords.extend(list(tmp))\n",
    "    \n",
    "# stopwords.append(\"court\")\n",
    "# stopwords.append(\"judge\")\n",
    "# stopwords.append(\"advocate\")\n",
    "# stopwords.append(\"general\")\n",
    "# stopwords.append(\"applicant\")\n",
    "# stopwords.append(\"defendant\")\n",
    "# stopwords.append(\"defendent\")\n",
    "# stopwords.append(\"article\")\n",
    "# stopwords.append(\"law\")\n",
    "# stopwords.append(\"preliminary\")\n",
    "# stopwords.append(\"ruling\")\n",
    "# stopwords.append(\"judgement\")\n",
    "# stopwords.append(\"judgment\")\n",
    "# stopwords.append(\"decision\")\n",
    "# stopwords.append(\"proceedings\")\n",
    "# stopwords.append(\"grand\")\n",
    "# stopwords.append(\"chamber\")\n",
    "# stopwords.append(\"paragraph\")\n",
    "# stopwords.append(\"subparagraph\")\n",
    "# stopwords.append(\"directive\")\n",
    "# stopwords.append(\"provision\")\n",
    "# stopwords.append(\"provisions\")\n",
    "# stopwords.append(\"interpreted\")\n",
    "# stopwords.append(\"interpretation\")\n",
    "# stopwords.append(\"council\")\n",
    "# stopwords.append(\"virtue\")\n",
    "# stopwords.append(\"regarding\")\n",
    "# stopwords.append(\"composed\")\n",
    "# stopwords.append(\"submitted\")\n",
    "# stopwords.append(\"behalf\")\n",
    "# stopwords.append(\"acting\")\n",
    "# stopwords.append(\"concerns\")\n",
    "# stopwords.append(\"request\")\n",
    "# stopwords.append(\"registrar\")\n",
    "# stopwords.append(\"president\")\n",
    "# stopwords.append(\"section\")\n",
    "# stopwords.append(\"preamble\")\n",
    "# stopwords.append(\"legislation\")\n",
    "# stopwords.append(\"question\")\n",
    "# stopwords.append(\"operative\")\n",
    "# stopwords.append(\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removes unnecessary stuff from text\n",
    "# def preprocess(doc):\n",
    "#     global stopwords\n",
    "#     doc = doc.lower()  # Lower the text.\n",
    "#     doc = word_tokenize(doc)  # Split into words.\n",
    "#     doc = [w for w in doc if not w in stopwords]  # Remove stopwords.\n",
    "#     doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "#     return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Standard cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosinedistance = model.similarity(\"healthcare\", \"medical\")  # Compute cosine similarity\n",
    "# print(\"cosine: \" + str(cosinedistance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Word Moving Distance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # wmddistance = model.wmdistance(\"healthcare\",\"medical\")  # Compute WMD similarity\n",
    "# # # print(\"wmd: \" + str(wmddistance))\n",
    "# from gensim.models import Word2Vec\n",
    "# # Train Word2Vec on all the EURLEX cases.\n",
    "# model = Word2Vec(values, workers=3, size=100)\n",
    "# model.save(\"wmdword2vec.model\")\n",
    "# # print(model)\n",
    "\n",
    "# # # Initialize WmdSimilarity.\n",
    "# # from gensim.similarities import WmdSimilarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similar(tfidf_matrix, index, top_n = 5):\n",
    "#     cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "#     related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "#     return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]\n",
    "\n",
    "# # Keep a record of document to index\n",
    "# def get_doc_index(docid):\n",
    "#     global key\n",
    "#     global tfidf_data\n",
    "#     rowid = key[docid]\n",
    "#     return rowid\n",
    "    \n",
    "# # Keep a record of document to index\n",
    "# def get_doc_row(docid):\n",
    "#     global key\n",
    "#     global tfidf_data\n",
    "#     rowid = key[docid]\n",
    "#     row = tfidf_data[rowid,:]\n",
    "#     return row\n",
    "\n",
    "# # Keep a record of document to index\n",
    "# def get_doc_id(rowid):\n",
    "#     global key\n",
    "#     global tfidf_data\n",
    "#     for k, v in key.items():    \n",
    "#         if v == rowid:\n",
    "#             return k\n",
    "#     return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Celex numbers of reference cases\n",
    "# publichealth = ['62004CJ0372','62003CJ0380','62003CJ0453','62011CJ0625','62011CJ0308','62015CJ0019','62008CJ0248','62013CJ0056','62010CJ0381']\n",
    "# socialpolicy = ['62007CJ0378','61988CJ0262','61990CJ0006','62007CJ0396','62010CJ0108','62012CO0178','62014CJ0238','62010CJ0305','62003CJ0333']\n",
    "# dataprotection = ['62006CJ0275','62001CJ0101','62010CJ0070','61998CJ0223','62014CJ0230','62008CJ0028','62006CJ0301','62004CO0318','62004CO0317']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.similarities import WmdSimilarity\n",
    "# from nltk import word_tokenize\n",
    "# num_best = 5\n",
    "# instance = WmdSimilarity(file_dict['62004CJ0372'], model, num_best=5)\n",
    "# query = preprocess(file_dict['62004CJ0372'])\n",
    "# sims = instance[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Query:')\n",
    "# #print(file_dict['62004CJ0372'])\n",
    "# for i in range(num_best):\n",
    "#     print()\n",
    "#     print('sim = %.4f' % sims[i][0])\n",
    "#     print(key[sims[i][0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8